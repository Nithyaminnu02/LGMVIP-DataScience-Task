{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "DataScience task 3 (Next word prediction)"
      ],
      "metadata": {
        "id": "IcQGdQzKJkQI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "agkV4lZPHhCU",
        "outputId": "63acc286-ad39-4c2f-c533-3dda96bc1573"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.25.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.11.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.0)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Sample text data\n",
        "text = \"\"\"\n",
        "You may say I'm a dreamer, but I'm not the only one. I hope someday you'll join us. And the world will live as one.\n",
        "\"\"\"\n",
        "\n",
        "# Tokenize the text\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([text])\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Create input sequences\n",
        "input_sequences = []\n",
        "for line in text.split(\"\\n\"):\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "    for i in range(1, len(token_list)):\n",
        "        n_gram_sequence = token_list[:i+1]\n",
        "        input_sequences.append(n_gram_sequence)\n",
        "\n",
        "# Pad sequences and create predictors and labels\n",
        "max_sequence_len = max([len(x) for x in input_sequences])\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "\n",
        "X, y = input_sequences[:,:-1], input_sequences[:,-1]\n",
        "y = tf.keras.utils.to_categorical(y, num_classes=total_words)\n"
      ],
      "metadata": {
        "id": "pOoywlfVHld0"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\n",
        "model.add(Bidirectional(LSTM(150, return_sequences=True)))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# View model summary\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bVyCTT72HlaW",
        "outputId": "49c964fd-a1de-4b9b-c89a-2ba1d00d8bdf"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 24, 100)           2300      \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirecti  (None, 24, 300)           301200    \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " lstm_3 (LSTM)               (None, 100)               160400    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 23)                2323      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 466223 (1.78 MB)\n",
            "Trainable params: 466223 (1.78 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "history = model.fit(X, y, epochs=100, verbose=1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XfbWL0zSHyUe",
        "outputId": "163c80ac-1735-4e7d-eff4-25fddb7b8b68"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1/1 [==============================] - 6s 6s/step - loss: 3.1364 - accuracy: 0.0833\n",
            "Epoch 2/100\n",
            "1/1 [==============================] - 0s 211ms/step - loss: 3.1248 - accuracy: 0.0833\n",
            "Epoch 3/100\n",
            "1/1 [==============================] - 0s 197ms/step - loss: 3.1125 - accuracy: 0.1667\n",
            "Epoch 4/100\n",
            "1/1 [==============================] - 0s 186ms/step - loss: 3.0972 - accuracy: 0.1667\n",
            "Epoch 5/100\n",
            "1/1 [==============================] - 0s 218ms/step - loss: 3.0765 - accuracy: 0.1250\n",
            "Epoch 6/100\n",
            "1/1 [==============================] - 0s 218ms/step - loss: 3.0468 - accuracy: 0.1250\n",
            "Epoch 7/100\n",
            "1/1 [==============================] - 0s 209ms/step - loss: 3.0045 - accuracy: 0.1250\n",
            "Epoch 8/100\n",
            "1/1 [==============================] - 0s 200ms/step - loss: 2.9528 - accuracy: 0.1250\n",
            "Epoch 9/100\n",
            "1/1 [==============================] - 0s 186ms/step - loss: 2.9245 - accuracy: 0.1250\n",
            "Epoch 10/100\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 2.8943 - accuracy: 0.1250\n",
            "Epoch 11/100\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 2.8285 - accuracy: 0.0833\n",
            "Epoch 12/100\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 2.7670 - accuracy: 0.1250\n",
            "Epoch 13/100\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 2.7132 - accuracy: 0.1250\n",
            "Epoch 14/100\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 2.6452 - accuracy: 0.1667\n",
            "Epoch 15/100\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 2.5667 - accuracy: 0.2083\n",
            "Epoch 16/100\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 2.4947 - accuracy: 0.2083\n",
            "Epoch 17/100\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 2.4116 - accuracy: 0.1667\n",
            "Epoch 18/100\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 2.3396 - accuracy: 0.1667\n",
            "Epoch 19/100\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 2.2769 - accuracy: 0.1667\n",
            "Epoch 20/100\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 2.2205 - accuracy: 0.2083\n",
            "Epoch 21/100\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 2.1283 - accuracy: 0.2083\n",
            "Epoch 22/100\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 2.1078 - accuracy: 0.2083\n",
            "Epoch 23/100\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 2.1293 - accuracy: 0.1667\n",
            "Epoch 24/100\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 1.9497 - accuracy: 0.2917\n",
            "Epoch 25/100\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 2.0270 - accuracy: 0.2500\n",
            "Epoch 26/100\n",
            "1/1 [==============================] - 0s 95ms/step - loss: 1.8421 - accuracy: 0.4583\n",
            "Epoch 27/100\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 1.8805 - accuracy: 0.3750\n",
            "Epoch 28/100\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 1.7623 - accuracy: 0.4583\n",
            "Epoch 29/100\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 1.6904 - accuracy: 0.4583\n",
            "Epoch 30/100\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 1.6722 - accuracy: 0.5000\n",
            "Epoch 31/100\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 1.5800 - accuracy: 0.5417\n",
            "Epoch 32/100\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 1.5890 - accuracy: 0.3333\n",
            "Epoch 33/100\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 1.7483 - accuracy: 0.4583\n",
            "Epoch 34/100\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 1.4589 - accuracy: 0.7917\n",
            "Epoch 35/100\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 1.6944 - accuracy: 0.3333\n",
            "Epoch 36/100\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 1.3909 - accuracy: 0.7500\n",
            "Epoch 37/100\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 1.5841 - accuracy: 0.3333\n",
            "Epoch 38/100\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 1.3257 - accuracy: 0.7917\n",
            "Epoch 39/100\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 1.4215 - accuracy: 0.5417\n",
            "Epoch 40/100\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 1.2594 - accuracy: 0.7917\n",
            "Epoch 41/100\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 1.3546 - accuracy: 0.4583\n",
            "Epoch 42/100\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 1.2306 - accuracy: 0.8333\n",
            "Epoch 43/100\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 1.2344 - accuracy: 0.7083\n",
            "Epoch 44/100\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 1.1852 - accuracy: 0.7083\n",
            "Epoch 45/100\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 1.1081 - accuracy: 0.8333\n",
            "Epoch 46/100\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 1.1249 - accuracy: 0.8333\n",
            "Epoch 47/100\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 1.0457 - accuracy: 0.8750\n",
            "Epoch 48/100\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 1.0476 - accuracy: 0.7500\n",
            "Epoch 49/100\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 1.0100 - accuracy: 0.8750\n",
            "Epoch 50/100\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 0.9730 - accuracy: 0.8750\n",
            "Epoch 51/100\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 0.9607 - accuracy: 0.8333\n",
            "Epoch 52/100\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 0.9150 - accuracy: 0.8750\n",
            "Epoch 53/100\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 0.8943 - accuracy: 0.9167\n",
            "Epoch 54/100\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 0.8806 - accuracy: 0.9583\n",
            "Epoch 55/100\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 0.8390 - accuracy: 0.9583\n",
            "Epoch 56/100\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 0.8128 - accuracy: 0.9583\n",
            "Epoch 57/100\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 0.8121 - accuracy: 0.9583\n",
            "Epoch 58/100\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 0.8205 - accuracy: 0.9583\n",
            "Epoch 59/100\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 0.9801 - accuracy: 0.7500\n",
            "Epoch 60/100\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 1.4545 - accuracy: 0.5000\n",
            "Epoch 61/100\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 1.1069 - accuracy: 0.6250\n",
            "Epoch 62/100\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 1.3383 - accuracy: 0.4167\n",
            "Epoch 63/100\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 1.2251 - accuracy: 0.5833\n",
            "Epoch 64/100\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 0.8899 - accuracy: 0.8750\n",
            "Epoch 65/100\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 1.1291 - accuracy: 0.6250\n",
            "Epoch 66/100\n",
            "1/1 [==============================] - 0s 119ms/step - loss: 1.0139 - accuracy: 0.6250\n",
            "Epoch 67/100\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 0.8774 - accuracy: 0.8333\n",
            "Epoch 68/100\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 0.9003 - accuracy: 0.7500\n",
            "Epoch 69/100\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 0.8506 - accuracy: 0.8333\n",
            "Epoch 70/100\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 0.8099 - accuracy: 0.8750\n",
            "Epoch 71/100\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 0.7769 - accuracy: 1.0000\n",
            "Epoch 72/100\n",
            "1/1 [==============================] - 0s 190ms/step - loss: 0.7704 - accuracy: 0.9583\n",
            "Epoch 73/100\n",
            "1/1 [==============================] - 0s 201ms/step - loss: 0.7545 - accuracy: 0.9167\n",
            "Epoch 74/100\n",
            "1/1 [==============================] - 0s 208ms/step - loss: 0.6797 - accuracy: 0.9583\n",
            "Epoch 75/100\n",
            "1/1 [==============================] - 0s 202ms/step - loss: 0.6853 - accuracy: 1.0000\n",
            "Epoch 76/100\n",
            "1/1 [==============================] - 0s 202ms/step - loss: 0.6479 - accuracy: 1.0000\n",
            "Epoch 77/100\n",
            "1/1 [==============================] - 0s 191ms/step - loss: 0.6269 - accuracy: 1.0000\n",
            "Epoch 78/100\n",
            "1/1 [==============================] - 0s 202ms/step - loss: 0.6019 - accuracy: 1.0000\n",
            "Epoch 79/100\n",
            "1/1 [==============================] - 0s 206ms/step - loss: 0.5875 - accuracy: 1.0000\n",
            "Epoch 80/100\n",
            "1/1 [==============================] - 0s 195ms/step - loss: 0.5660 - accuracy: 1.0000\n",
            "Epoch 81/100\n",
            "1/1 [==============================] - 0s 187ms/step - loss: 0.5391 - accuracy: 1.0000\n",
            "Epoch 82/100\n",
            "1/1 [==============================] - 0s 187ms/step - loss: 0.5258 - accuracy: 1.0000\n",
            "Epoch 83/100\n",
            "1/1 [==============================] - 0s 180ms/step - loss: 0.5154 - accuracy: 1.0000\n",
            "Epoch 84/100\n",
            "1/1 [==============================] - 0s 209ms/step - loss: 0.5030 - accuracy: 1.0000\n",
            "Epoch 85/100\n",
            "1/1 [==============================] - 0s 212ms/step - loss: 0.4842 - accuracy: 1.0000\n",
            "Epoch 86/100\n",
            "1/1 [==============================] - 0s 210ms/step - loss: 0.4643 - accuracy: 1.0000\n",
            "Epoch 87/100\n",
            "1/1 [==============================] - 0s 192ms/step - loss: 0.4530 - accuracy: 1.0000\n",
            "Epoch 88/100\n",
            "1/1 [==============================] - 0s 225ms/step - loss: 0.4402 - accuracy: 1.0000\n",
            "Epoch 89/100\n",
            "1/1 [==============================] - 0s 219ms/step - loss: 0.4292 - accuracy: 1.0000\n",
            "Epoch 90/100\n",
            "1/1 [==============================] - 0s 249ms/step - loss: 0.4159 - accuracy: 1.0000\n",
            "Epoch 91/100\n",
            "1/1 [==============================] - 0s 221ms/step - loss: 0.4033 - accuracy: 1.0000\n",
            "Epoch 92/100\n",
            "1/1 [==============================] - 0s 230ms/step - loss: 0.3945 - accuracy: 1.0000\n",
            "Epoch 93/100\n",
            "1/1 [==============================] - 0s 204ms/step - loss: 0.3797 - accuracy: 1.0000\n",
            "Epoch 94/100\n",
            "1/1 [==============================] - 0s 207ms/step - loss: 0.3704 - accuracy: 1.0000\n",
            "Epoch 95/100\n",
            "1/1 [==============================] - 0s 180ms/step - loss: 0.3585 - accuracy: 1.0000\n",
            "Epoch 96/100\n",
            "1/1 [==============================] - 0s 209ms/step - loss: 0.3498 - accuracy: 1.0000\n",
            "Epoch 97/100\n",
            "1/1 [==============================] - 0s 199ms/step - loss: 0.3414 - accuracy: 1.0000\n",
            "Epoch 98/100\n",
            "1/1 [==============================] - 0s 197ms/step - loss: 0.3308 - accuracy: 1.0000\n",
            "Epoch 99/100\n",
            "1/1 [==============================] - 0s 192ms/step - loss: 0.3218 - accuracy: 1.0000\n",
            "Epoch 100/100\n",
            "1/1 [==============================] - 0s 206ms/step - loss: 0.3132 - accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def predict_next_word(model, tokenizer, text, max_sequence_len):\n",
        "    token_list = tokenizer.texts_to_sequences([text])[0]\n",
        "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "    predicted = model.predict(token_list, verbose=0)\n",
        "    predicted_word_index = np.argmax(predicted, axis=-1)\n",
        "    predicted_word = tokenizer.index_word[predicted_word_index[0]]\n",
        "    return predicted_word\n",
        "\n",
        "# Example usage\n",
        "seed_text = \"You may say I'm\"\n",
        "next_words = 10\n",
        "\n",
        "for _ in range(next_words):\n",
        "    next_word = predict_next_word(model, tokenizer, seed_text, max_sequence_len)\n",
        "    seed_text += \" \" + next_word\n",
        "\n",
        "print(seed_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktk_YEEwH4FW",
        "outputId": "ef69985f-7320-4c51-caa9-bd994a18ec60"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You may say I'm a dreamer but i'm not the only one i hope\n"
          ]
        }
      ]
    }
  ]
}